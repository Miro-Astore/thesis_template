%=======================================================================================%
\chapter{Theory and Methods}
\numberwithin{equation}{chapter}
\label{chap:methods}

\chapquote{``In the real world, this could eventually mean that most chemical experiments 
are conducted inside the silicon of chips instead of the glassware of laboratories. Turn 
off that Bunsen burner; it will not be wanted in ten years."}{The Economist, on the 1998 
Chemistry Nobel Prize Awardees \\ Walter Kohn and John Pople}

\section{Quantum Mechanical Origins}
For a microscopic system, the nature of the interactions between particles/atoms is 
fundamentally quantum mechanical. This quantum mechanical system contains both nuclei $N_{n}$ 
and electrons $N_{e}$. Suppose $i$ and $\alpha$ represent the indices for individual nuclei 
and electrons, then the coordinates can then be written as ${\bf R}_{i}$ and ${\bf r}_{\alpha}$ 
respectively. The interaction of this many-body system is described with the time-dependent 
Schr\"{o}dinger equation
\begin{equation}
i\hbar\frac{\partial}{\partial t}\Psi(\{{\bf R}_{i},{\bf r}_{\alpha}\},t) = H\Psi(\{{\bf R}_{i},{\bf r}_{\alpha}\},t),
\end{equation}
where $\Psi$ and $H$ is the wave function and Hamiltonian of the nuclei-electron system, 
respectively. Solving this equation can be extremely complicated if not impossible; thus, 
some approximations are required. Instead of solving the equation above the time-independent 
Schr\"{o}dinger equation can be solved instead
\begin{equation}
H\Psi(\{{\bf R}_{i},{\bf r}_{\alpha}\}) = E\Psi(\{{\bf R}_{i},{\bf r}_{\alpha}\}),
\end{equation}
where $E$ is the total energy. This is valid only for stationary states (i.e. excluding 
excited states). The Hamiltonian for the system of interacting nuclei and electrons is 
written as the sum of several components
\begin{equation}
H = T_{n} + T_{e} + V_{n\text{-}n} + V_{e\text{-}e} + V_{n\text{-}e}.
\end{equation}
The first two terms are the kinetic energies of nuclei and electrons, and the last three terms 
are the nucleus-nucleus, electron-electron and nucleus-electron potential energy. The potential 
energies can be represented as Coulomb interactions, and thus the individual terms above are 
expanded to the set of equations below,
\begin{align}
T_{n} &= -\sum_{i} \frac{\hbar^2}{2M_{i}}\nabla_{i}^2, & V_{n\text{-}n} &= \sum_{i>j} \frac{z_{i}z_{j}e^2}{|{\bf R}_{i} - {\bf R}_{j}|}, \nonumber \\ 
T_{e} &= -\sum_{\alpha} \frac{\hbar^2}{2m}\nabla_{\alpha}^2,  & V_{e\text{-}e} &= \sum_{\alpha>\beta} \frac{e^2}{|{\bf r}_{\alpha} - {\bf r}_{\beta}|}, \nonumber \\
& & V_{n\text{-}e} &= \sum_{i,\alpha} \frac{z_{i}e^2}{|{\bf R}_{i}-{\bf r}_{\alpha}|}. \label{eq:HamFull}
\end{align}
Here $M_{i}$ and $m$ are the masses of the nuclei and electrons, respectively. The total 
wave function $\Psi(\{{\bf R}_{i},{\bf r}_{\alpha}\})$ can be separated into the product 
of the nuclei $\Psi_{n}(\{{\bf R}_{i}\})$ and electrons $\Psi_{e}(\{{\bf R}_{i},{\bf r}_{\alpha}\})$ 
wave functions. This decoupling is valid since the nuclei are much heavier than the electrons, 
so the motion of the nuclei is independent of the electrons. This \textit{ansatz} is known as 
the adiabatic or Born-Oppenheimer approximation~\cite{Born1927}. However, the motion of the 
electrons will still be dependent on the nuclei due to the electron being localised to the 
nuclei. With this approximation, the time-independent Schr\"{o}dinger equation for the 
electrons and nuclei respectively are
\begin{subequations}
\begin{align}
[T_{e} + V_{e\text{-}e} + V_{n\text{-}e}]\Psi(\{{\bf R}_{i},{\bf r}_{\alpha}\}) &= E_{e}(\{{\bf R}_{i}\})\Psi_{e}(\{{\bf R}_{i}, {\bf r}_{\alpha}\}), \label{eq:Ham1}\\
[T_{n} + V_{n\text{-}n} + E_{e}(\{{\bf R}_{i}\})]\Psi(\{{\bf R}_{i}\}) &= E\Psi_{n}(\{{\bf R}_{i}\}. \label{eq:Ham2}
\end{align}
\end{subequations}
Eq.~\eqref{eq:Ham1} is the starting point for any {\it ab initio} electronic calculations 
and $E_{e}(\{{\bf R}_{i}\})$ is the total energy of the electrons (usually taken at the 
ground state). To further simplify the problem, the nuclei interactions are treated classically. 
Following the definition of the conservative force $F = -\nabla V$ (force depending only on 
position) the motion of the nuclei is dictated by Newton's equation of motion
\begin{equation}
M_{i}{\bf \ddot{R}}_{i} = -\nabla_{i}V(\{{\bf R}_{i}\}),
\label{eq:motion}
\end{equation}
where $V(\{{\bf R}_{i}\})$ is a potential energy function. Using the definitions from 
Eqs.~\eqref{eq:HamFull} and \eqref{eq:Ham2} the potential energy function that describes nuclei 
interactions is
\begin{equation}
V({{\bf R}_{i}}) = \sum_{i>j} \frac{z_{i}z_{j}e^2}{|{\bf R}_{i} - {\bf R}_{j}|} + E_{e}(\{{\bf R}_{i}\}),
\label{eq:potfunc}
\end{equation}
which is the nucleus-nucleus Coulomb interaction plus the ground state electronic energy. The 
potential energy function above thus contains both classical and quantum mechanical description. 
Eqs.~\eqref{eq:Ham1}, \eqref{eq:motion} and \eqref{eq:potfunc} are the foundations for {\it ab 
initio} molecular dynamics (AIMD) simulations. Although AIMD gives a very accurate description 
of the microscopic interactions, simulating biomolecular systems (consisting of ${\sim}10^5$ atoms) 
is still unreachable with today's computing hardware. This is true even with the density functional 
theory (DFT) approximation~\cite{Kohn1965,Rajagopal1973} coupled with Car-Parrinello MD 
(CPMD)~\cite{Car1985}, which is the ``state-of-the-art" AIMD method. CPMD can simulate system sizes 
of ${\sim}10^3$ atoms but only reaching time scales of ${\sim}10^2$ picoseconds with the current 
hardware. Therefore, a classical approximation of the electronic interactions Eq.~\eqref{eq:potfunc} 
is needed for simulating biomolecular systems.

\section{Classical Molecular Dynamics}
\label{sec:cmd}
The quantum mechanical part of Eq.~\eqref{eq:potfunc} can be decomposed into three components: 
{\it repulsion}, {\it dispersion} and {\it polarisation}. Electron repulsion is a consequence 
of the Pauli exclusion principle where two electrons cannot occupy the same quantum state. The 
repulsive force is short-range and ensures that electron orbitals do not overlap. Dispersion 
forces are attractive and arise from weak dipole-dipole interactions (van der Waals or London 
dispersion forces) that have a $1/r^6$ dependence. Lastly, polarisation is the ability of 
electron orbitals to be distorted under the influence of an external electric field. All three 
of these interactions can be approximated classically through the use of empirically fitted 
analytical functions. 

\subsection{Empirical Force Fields}
\label{sec:forcefield}
Most MD simulations of biomolecules employ a force field to describe the potential energy 
function of the system. Some commonly used force field packages include CHARMM~\cite{MacKerell1998}, 
AMBER~\cite{Hornak2006}, OPLS~\cite{Jorgensen1996} and GROMOS~\cite{Horta2011}. These force 
field packages describe atomic interactions using analytical functions that are fitted to 
some target data. The target data may be from experiment or accurate {\it ab initio} 
calculations (or a mixture of both). In this thesis, all simulations utilise the CHARMM 
additive force field. Firstly, the electrostatic interactions between two atoms $i$ and $j$ 
are calculated with Coulomb's law
\begin{equation}
V_{Elec}(r_{ij}) = k_{c}\frac{q_{i}q_{j}}{r_{ij}},
\end{equation}
where $k_{c}$ is the Coulomb constant. This potential is essentially the classical part of 
Eq.~\eqref{eq:potfunc}.  For the quantum mechanical part, the repulsive and dispersive forces 
can be combined into one equation known as the 12-6 Lennard-Jones (LJ) potential
\begin{equation}
V_{LJ}(r_{ij}) = \epsilon_{ij}\left[\left(\frac{R_{\text{min}_{ij}}}{r_{ij}}\right)^{12} - 2\left(\frac{R_{\text{min}_{ij}}}{r_{ij}}\right)^{6} \right],
\label{eq:LJ}
\end{equation}
where $\epsilon_{ij}$ is the well depth and $R_{\text{min}_{ij}}$ is the position of the 
potential at the minimum. In the literature, the LJ potential is usually written terms of 
$\sigma_{ij}$, which is the distance at which the energy is zero, and the $\epsilon_{ij}$ 
has a constant of 4 (the constant of 2 inside is removed). The relationship between the two 
distances is $R_{\text{min}}=2^{1/6}\sigma$. While the dispersive term comes out of quantum 
mechanics, the $1/r^{12}$ dependence for the repulsive term was adopted for simplicity. This 
term was chosen due to the hardware available when John Lennard-Jones first proposed the 
potential function, and it is easier to calculate $1/r^{12}$ than some other term because 
it is the square of $1/r^{6}$.\footnote{Some force fields like the AMOEBA force field uses 
a 14-7 LJ~\cite{Ponder2010}.} The two constants in the 
equation above are parameters that need to be optimised for each atom type. The LJ parameters 
between pairs of atoms are combined using the Lorentz-Berthelot combination rule~\cite{Allen1987}. 
This rule states that for any pairs of atoms, $\epsilon_{ij}$ and $R_{\text{min}_{ij}}$ is 
equal to the geometric and arithmetic mean of the individual components, respectively. 
Mathematically this is expressed as 
\begin{equation}
\epsilon_{ij} = \sqrt{\epsilon_{i}\epsilon_{j}},\quad\quad R_{\text{min}_{ij}} = \frac{1}{2} (R_{\text{min}_{i}} + R_{\text{min}_{j}}).
\end{equation}
The rule above is based on the hard-sphere model of particles and prevents any overlap 
between atoms. \figref{method:LJ} illustrates the dual nature of the LJ potential (dispersive 
and repulsive). The effect of polarisation is not included in the CHARMM additive force field; 
hence, it is classified as a non-polarisable force field. Polarisation in classical force fields 
can be included by using the classical Drude model of charge on a spring or direct induced 
polarisation with multipole expansion~\cite{Baker2015}. Packages that include explicit 
polarisation using these methods are CHARMM Drude force field~\cite{Lopes2013} and the AMOEBA 
force field~\cite{Ponder2010}. Although polarisable force fields are available, they are 
generally more computationally expensive than non-polarisable force fields (though the implementation 
of the Drude polarisable force field in NAMD only increases the computational cost not more than 
a factor of two compared to non-polarisable force fields~\cite{Jiang2011a}). 
Also, compared to  polarisable force fields, non-polarisable force field has been in development 
since the 1980s. Thus the validity of their chemical accuracy is better understood than polarisable 
force fields. The potentials $V_{Elec}$ and $V_{LJ}$ describe interactions between pairs of atoms; 
however, they do not capture the topology of molecules. Thus the sum of these two potentials is 
referred to as {\it non-bonded} interactions, $V_{nonbonded}$. 
\begin{figure}[t!]
\centering
\begin{minipage}{0.4\textwidth}
\centering
\includegraphics[width=5.0cm]{Figures/Method/Lennard-Jones-1.png}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{Figures/Method/Lennard-Jones.png}
\end{minipage}
\caption{(\textit{Left}) Two different atoms separated by $r_{ij}$ and (\textit{Right}) 
the LJ potential between the two atoms. The total potential is the sum of the repulsive 
and dispersive (attractive) terms. (Image on the left is designed in Tikz and the plot 
on the right is generated in Matlab)}
\label{method:LJ}
\end{figure}

For molecules, extra potential energy functions are needed to describe the many-body interactions. 
The potential energy function of Eq.~\eqref{eq:motion} can be expanded as an infinite series
\begin{equation}
V(\{r_{i}\}) = \sum_{i>j}V_{\text{2-body}}(r_{i},r_{j}) + \sum_{i>j>k}V_{\text{3-body}}(r_{i},r_{j},r_{k}) + \sum_{i>j>k>l}V_{\text{4-body}}(r_{i},r_{j},r_{k},r_{l}) + ...
\end{equation}
Almost all force field packages available include the expansion above only up to the fourth 
term to simplify the problem. The 2-body interactions, i.e. bonds between two atoms, is 
described with a harmonic oscillator (\figrefi{method:bonded}{A}) with a potential energy 
function in the form of
\begin{equation}
V_{bonds} = K_{b}(b-b_{0})^2,
\end{equation}
where $b$ is the bond length (i.e. $|r_{i}-r_{j}|$) and $b_{0}$ is the equilibrium position. 
The spring constant above contains the factor of $1/2$ as per the convention used in CHARMM 
(i.e. $K = k/2$)\footnote{This Convention is kept from the 1980s since it removes the need 
to calculate $k/2$, reducing the amount of unnecessary computation at each integration step.}. 

The 3-body interactions describe the bending of two bonds formed between three atoms. For 
example, the H-O-H angle of a water molecule is 104.5$^{\circ}$, and a potential function is 
required to preserve the shape of the molecule. In CHARMM there are two different types the 
3-body potentials as shown below
\begin{equation}
V_{angles} = K_{\theta}(\theta-\theta_{0})^2 + K_{ub}(S-S_{0})^2,
\end{equation}
where $\theta$ is the angle between atoms $i$-$j$-$k$, $S$ is the distance between atoms 
$i$-$k$ and $\theta_{0}$ and $S_{0}$ are the respective equilibrium positions. The first 
term is a harmonic potential applied on the angle between two bonds as illustrated in 
\figrefi{method:bonded}{B}, while the second part is a harmonic potential applied to the 
distance between atoms 1 and 3. This distance potential is known as the {\it Urey-Bradley} 
potential, which is used to include angle bending in some molecules. 

The 4-body interactions are called dihedral potentials, which determines the flexibility 
of molecules. The dihedral potentials used in CHARMM is in the form of
\begin{figure}[b!]
\centering
\includegraphics[width=12cm]{Figures/Method/MolecularMechanics.png}
\caption{Bonded terms describing the (A) 2-body bonds (B) 3-body angle and 4-body 
(C) proper and (D) improper dihedral interactions used in force fields. (Images 
are designed in Tikz)}
\label{method:bonded}
\end{figure}
\begin{equation}
V_{dihedrals} = K_{proper}(1+cos(n\phi - \delta)) + K_{improper}(\varphi-\varphi_{0})^2.
\end{equation}
Here $n$ is the multiplicity, $\delta$ is the phase angle, and $\phi$ and $\varphi$ are the 
{\it proper} and {\it improper} dihedral angles respectively. The first potential above describes 
proper dihedrals, which tries to capture molecular conformations that is a consequence of 
$\pi$-bonding, etc., and the constant $K_{proper}$ is the energy barrier height instead of a 
spring constant. Without proper dihedrals, molecules like polymers would not be able to rotate 
or change conformations. The second 4-body potential is the improper dihedrals, which keeps 
a group of atoms flat on a plane. One example where this might be used is in a benzene 
molecule. Since the carbon atoms are $sp^{2}$ hybridised, the benzene molecule must be planar 
and improper dihedrals, try to mimic this behaviour. The dihedral potentials are illustrated in 
\figrefn{method:bonded}{C{\color{black}--}D}. The sum of the bond, angles and dihedral potentials 
is equal to the \textit{bonded} potential $V_{bonded}$ that describes the topology of a molecule. 
The potential energy function that is used in Eq.~\eqref{eq:motion} to replace the quantum 
mechanical potential is
\begin{align}
V_{System} = V_{bonded} &+ V_{nonbonded}, \nonumber\\
\begin{split}
V_{bonded}=\begin{cases} V_{bonds}(b) \\ 
V_{angles}(\theta,S) \\ 
V_{dihedrals}(\phi,\psi)\end{cases} &+ \quad
V_{nonbonded}=\begin{cases}V_{LJ}(r_{ij})
\\ V_{Elec}(r_{ij})\end{cases}
\end{split}
\end{align}
Newton's equation of motion is numerically integrated with the potential energy function above, 
and, if appropriately parametrised, should be able to describe biomolecular systems accurately 
in comparison to experiment or {\it ab initio} calculations.\footnote{Although some systems 
of interest may require the explicit treatment of polarisation.}

\subsection{Numerical Integration}
Given the potential energy function of the system, we can determine the force acting on each 
atom. If the force on each atom is known, then we can determine the motion of each atom 
following Newton's equation of motion. Using Newton's second law and the conservative force 
we arrive at
\begin{equation}
F_{i} = m_{i}\frac{d^2 r_{i}}{dt^2} = -\nabla_{i} V({{\bf r}_{i}}),
\label{eq:newton}
\end{equation}
where $m$ is the mass of atom $i$ and $r_{i}$ is the current position of the atom. The positions 
and velocities of each atom are obtained by integrating the acceleration $a_{i} = d^2 r_{i}/dt^2$.
The integration is performed numerically by discretising the time $dt$ with a small timestep 
$\Delta t$. The time step $\Delta t$ needs to be chosen carefully as a large integration step 
will lead to numerical instabilities. A time step of 1 fs is needed to capture the vibrational 
motion of hydrogen atoms, however, constraint algorithms like SHAKE~\cite{Ryckaert1977} and 
LINCS~\cite{Hess1997} reduces the hydrogen degrees of freedom (rigid water molecules) and a 
time step of 2 fs can be used instead. A further trick that can be used to increase the time 
step is to increase the mass of the hydrogen atoms. This is known as hydrogen mass repartitioning 
(HMR) and allows an increase in the time step to 4 fs~\cite{Hopkins2015}. However, the larger 
integration step is associated with a larger numerical error. All simulations performed in the 
investigations carried out in this thesis use a time step of 2 fs by employing the SHAKE algorithm.

There are a few computer algorithms that are used for numerically integrating the acceleration. 
The NAMD program uses an efficient {\it velocity-Verlet} method~\cite{Swope1982}. This method 
requires the evaluation of velocity twice, but only one calculation is needed for the position and 
force. The position, force and velocity is updated at every time step following the algorithm below
\begin{subequations}
\begin{align}
v_{n+\frac{1}{2}} &= v_{n} + \frac{F_{n}}{m}\frac{\Delta t}{2}, \\
r_{n+1} &= r_{n} + v_{n+\frac{1}{2}}\Delta t, \\
F_{n+1} &= F(r_{n+1}), \label{eq:forceupdate} \\
v_{n+1} &= v_{n+\frac{1}{2}}+\frac{F_{n+1}}{m}\frac{\Delta t}{2}.
\end{align}
\end{subequations}
The most expensive calculation in the steps above is the force evaluation~\eqref{eq:forceupdate} 
as this requires the computation of the interactions between all atoms. The computation time 
increases with a factor of $N^2$ where $N$ is the number of atoms. One way of reducing 
computation time is the use of a cut-off distance $r_{c}$ when evaluating atomic interactions. 
In all simulations performed in this thesis, a cut-off of 12~\angs\ with a switching distance 
$r_{s}$ of 10~\angs\ is used for the LJ interactions to limit the computational cost. A 
switching function replaces the potential with a smooth function so that the energy approaches 
zero at 12~\angs\ starting from 10~\angs\ (see \figref{method:cutoff} for an illustration). 
The use of a cut-off distance is justified because the LJ interactions drop by a factor of 
$1/r^6$, and beyond 12~\angs\ the LJ interactions become negligible. However, the Coulomb 
interactions have a $1/r$ dependence\footnote{This dependence is from the monopole term of 
point charges}, and a 12~\angs\ cut-off is too short to capture this interaction accurately. 
Thus instead of increasing the cut-off distance, the long-range electrostatic interactions are 
determined using the particle-mesh Ewald (PME) summation method~\cite{Darden1993}. This method 
requires the use of periodic boundary conditions (PBC). With PME, the simulation box is divided 
into a finite grid, and the charges in the system are mapped onto the grid points. The fast 
Fourier transform (FFT) method is used to calculate the long-range electrostatic force of the 
charges on the grid. Calculating the long-range electrostatic force this way gives an 
$N.\text{log}(N)$ scaling instead of $N^2$.
\begin{figure}[t!]
\centering
\begin{minipage}{0.4\textwidth}
\centering
\includegraphics[width=6cm]{Figures/Method/Cutoff-1.png}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\centering
\includegraphics[width=1.0\textwidth]{Figures/Method/Cut-Off.png}
\end{minipage}
\caption{(Left) The cut-off distance $r_{c}$ used in MD simulations to reduce 
computational cost. A switching function is used, starting from $r_{s}$ to $r_{c}$ 
to smooth the potential to zero (Right). (Image on the left is designed in Tikz 
and the plot on the right is generated in Matlab)}
\label{method:cutoff}
\end{figure}

\subsection{Thermodynamic States}
An important aspect to consider when running MD simulations is the thermodynamic state of the 
system. A thermodynamic state can be defined by parameters such as the number of particles $N$, 
temperature $T$, pressure $P$ and chemical potential $\mu$. Fixing these parameters, i.e. making 
them constant throughout the simulation, will set the thermodynamic state or the \textit{ensemble} 
of the system. During MD simulations, points are generated in phase space (momentum-position space) 
and the volume of these points gives the total number of states (permutations) of the system. 
Thus the ensemble of a system relates the possible permutation of the microscopic system to the 
observable macroscopic states. Integrating Eq.~\eqref{eq:newton} will intrinsically give a constant 
number of atoms $N$, volume $V$ and energy $E$ (NVE). MD simulations with this type of thermodynamic 
state will be in a {\it microcanonical} ensemble, which represents an isolated system. Since 
biomolecular systems in real life are not isolated, modification to the equation of motion is 
required to incorporate other thermodynamic states.

\subsubsection{Temperature Control}
Temperature is a human-derived quantity that is used for convenience to relate how "hot" or "cold" 
something is. In thermodynamics, however, the temperature is related to the average kinetic energy of 
all particles in the system. To keep the temperature constant in MD simulations, a thermostat is 
needed that "kicks" the particles back to the desired kinetic energy. One way to do this is to 
employ Langevin dynamics that is based on the Langevin equation
\begin{equation}
\frac{d^2 r_{i}}{dt^2} = \frac{F_{i,\text{sys}}(r)}{m_{i}} - \gamma \frac{d r_{i}}{dt} + \sqrt{\frac{2\gamma \kT}{h}}R_{i}(t) ,
\label{eq:nvt}
\end{equation}
where $F_{\text{sys}}(r)$ is the total force on an atom from all other atoms in the system, 
$\gamma$ is a friction coefficient, $h$ is Planck's constant and $R(t)$ is a univariate Gaussian 
noise. This equation is essentially a damped harmonic oscillator with random "kicks" that follows 
a Gaussian distribution. The noise term represents small Brownian particles that collide 
with the atoms in the system to push the temperature of the system (i.e. kinetic energy) to the 
set constant value. With the temperature held constant, the system running under Langevin dynamics 
is in the {\it canonical} ensemble (NVT).

\subsubsection{Pressure Control}
In order to control the pressure of the system, the volume of the box must change. This is 
necessary to run MD simulations in an isobaric-isothermal ensemble (NPT). The 
barostat\footnote{Analogous to thermostat for pressure} employed in NAMD is a hybrid 
Nos\'e-Hoover Langevin piston method~\cite{Feller1995}. In the Nos\'e-Hoover method, a fictitious 
particle $e$ is used to represent a ``piston" acting on the system box~\cite{Martyna1994}. 
The piston can compress and expand the box isotropically or anisotropically with an oscillation 
period of $\tau$. The motion of the piston is governed by the Langevin equation and thus the 
dynamics of the piston is given by 
\begin{subequations}
\begin{align}
\frac{d e}{dt} &= \frac{1}{3V}\frac{d V}{dt}, \\
\frac{d^2 e}{dt^2} &= \frac{3V}{W}(P-P_{0}) - \gamma_{e}\frac{d e}{dt} + \sqrt{\frac{2\gamma_{e}\kT}{h}}R_{e}(t), \\
W &= 3N \tau^2 k_{B}T,
\end{align}
\end{subequations}
where $V$ is the volume of the box, $P$ is the pressure, $W$ is the mass of the fictitious 
particle (i.e. the piston), and $R_{e} (t)$ is a similar Gaussian noise in Eq.\eqref{eq:nvt} 
for the piston. The beauty of this method is that only a small modification to the equation of 
motion is required for the velocity and acceleration terms. The velocity and acceleration of 
atoms are modified by $(de/dt)r$ and $(de/dt)v$ respectively. Therefore, the positions of each 
atom are scaled to the change in box dimensions automatically as a result of the piston motion.

The friction or damping coefficients $\gamma$ and $\gamma_{e}$ determines the coupling strength 
of the simulation to the thermostat and barostat, respectively. This parameter is set by the user 
in units of ps$^{-1}$ and the oscillation period $\tau$ scales by $1/\gamma_{e}$. This ensures 
that the piston dynamics is close to being critically damped. Using a large value for the decay 
might affect the properties like diffusion of molecules as it prioritises equilibration. Therefore, 
an appropriate value needs to be used to give a stable simulation run.

\section{Free Energy Calculations}
\label{sec:freenergy}
\chapquote{``Thermodynamics is a funny subject. The first time you go through it, you 
don't understand it at all. The second time you go through it, you think you understand 
it, except for one or two small points. The third time you go through it, you know you 
don't understand it, but by that time you are so used to it, it doesn't bother you any 
more."}{Arnold Sommerfeld}

\vskip 0.5cm

Simulations of biomolecules with MD can give some information about the behaviour and dynamics 
of the system. However, in most cases, the time scales of processes like protein-ligand binding, 
ion diffusion through ion channels and protein folding is unreachable with brute-force MD. This 
is where free energy calculations are used to extract useful information about the system. Free 
energy methods can give quantities like solvation and binding free energies, which can be directly 
compared to experiments. Such a tool is valuable for the validation of force field models/parameters 
or complements experimental results to elucidate some processes. There are many different ways of 
computing free energies, and they all can be classified into two categories 
\textit{path-independent} and \textit{path-dependent}. 

\subsection{Alchemical Perturbation}
Path-independent free energy methods are focused on obtaining the free energy difference between 
two states. For example, the binding free energy of a ligand to a protein host is equal to the 
change in free energy between the ligand in bulk water (point \textbf{A}) and inside the protein 
(point \textbf{B}). The path the ligand takes from \textbf{A} to \textbf{B} is not considered in 
the calculation; however, if the path is an essential factor in capturing the dynamics, then a 
path-dependent method is required (see \secref{sec:PMF}). To obtain the free energy between two 
points, the ligand or molecule is ``{\it alchemically}" transformed into a void or another species. 
This means that the nonbonded interactions of the molecule with the environment are switched on 
and off. The alchemical transformation is achieved by introducing a hybrid potential energy in the 
form of
\begin{equation}
U(\lambda) = (1-\lambda)U_{0} + \lambda U_{1},
\end{equation}
where $U_{0}$ and $U_{1}$ are the potential energies for the initial and final states, respectively. 
The variable $\lambda$ is a discreet coupling parameter that ranges from 0 to 1 and determines 
the degree of perturbation of the system. As an example, $\lambda=0$ may represent a positive 
ion surrounded by water, and $\lambda=1$ is the state where the ion is completely annihilated 
(illustrated in \figref{method:solv}). At each $\lambda$ value, the system is sampled with the 
modified potential energy. In practice, a certain amount of data is discarded as the equilibration 
phase\footnote{In Monte Carlo simulations this is referred as the "burn" phase} and simulation 
data after this is taken as the production data. Simulations that use this modified potential 
energy are referred collectively as free energy perturbation (FEP).
\begin{figure}[t!]
\centering
\includegraphics[width=0.85\textwidth]{Figures/Method/Solvation.png}
\caption{An ion surrounded by water molecules is alchemically destroyed 
starting from $\lambda=0$ to 1. (Image is designed in Tikz)}
\label{method:solv}
\end{figure}

\subsubsection{Exponential Averaging}
Given the modified potential energy of the system, the free energy can be obtained using several 
methods. The earliest method to obtain free energy of a perturbed system is {\it exponential 
averaging}\footnote{Also known as the Zwanzig relation} (EXP)~\cite{Beveridge1989}. In essence, 
at each time step of $\lambda_{i}$, the energy at both $\lambda_{i}$ and $\lambda_{i+1}$ is 
calculated. The energy at $\lambda_{i+1}$ is calculated with the coordinates of $\lambda_{i}$. 
The Boltzmann probability of the energy difference between these two states is averaged over the 
number of samples at each state. Then, the Gibbs free energy at $\lambda_{i}$ is given by
\begin{equation}
\Delta G_{i} = -\kT \,\text{ln} \left\langle \text{exp}\left[-\frac{U(\lambda_{i+1})-U(\lambda_{i})}{k_{B}T} \right]\right\rangle_{\lambda_{i}},
\label{eq:zwanzig}
\end{equation}
where $k_{\text{B}}$ is the Boltzmann constant and $T$ is the temperature. The sum gives the free 
energy of the whole transformation (i.e. $\lambda$ from 0 to 1)
\begin{equation}
\Delta G = \sum_{i}^{N} \Delta G_{i}.
\label{eq:sumG}
\end{equation}
Choosing the number of $\lambda$ points is tricky and is system dependent. For example, a previous 
study showed that 130 equally spaced $\lambda$ values were needed to obtain convergence for the 
free energy of translocating a charged ligand from the binding site in \GltPh\ to bulk 
water~\cite{Heinzelmann2011}. However, using exponentially spaced values instead reduces the number 
of windows to 66. For calculating solvation free energies of small molecules, however, a smaller number 
of $\lambda$ values is needed. Different strategies may be adopted to obtain convergence, and one recipe 
may not apply to all systems. The work in this thesis follows the strategy given in 
Ref~\cite{Heinzelmann2011} as the same system (i.e., \GltPh) is investigated.

\subsubsection{Thermodynamic Integration}
The large number of $\lambda$ values required with EXP can make it very expensive to compute. 
An alternate way to calculate free energy with the alchemical route is to use thermodynamic 
integration (TI)~\cite{Beveridge1989}. In this method, the ensemble average of the derivative 
$\partial U/\partial \lambda$ is calculated instead. The free energy difference is obtained by 
numerically integrating the derivative of the hybrid potential energy with respect to $\lambda$,
\begin{equation}
\Delta G = \int_{0}^{1} \left\langle \frac{\partial U(\lambda)}{\partial \lambda} \right\rangle_{\lambda} d\lambda.
\end{equation}
With TI, the number of $\lambda$ values can be significantly reduced. For small neutral molecules, 
11 windows are sufficient to capture the free energy profile accurately. For charged molecules, 
the free energy profile is quadratic\footnote{This comes from nature of Coulomb interaction, i.e. 
$q^2$}, and Gaussian quadrature can be used for the integration~\cite{press2007numerical}. 
Gaussian quadrature is a numerical method that approximates an integral with weighted sums of 
function values at the specified point, i.e. 
\begin{equation}
\int_{0}^{1} f(x) \approx \sum_{i}^{n} \omega_{i} f(x_{i}),
\end{equation} 
where $\omega_{i}$ is the weight value at a point $i$ and $f(x_{i})$ is the function value at $x_{i}$. 
Applying this to TI, the function $f(x_{i})$ represents the free energy derivative at $\lambda_{i}$. 
In a previous study of ion binding in gA with TI, a seven-point quadrature was found to be 
sufficient~\cite{Bastug2006c}. Therefore, more extensive sampling can be done for each $\lambda$ 
value allowing a more accurate estimate of the free energy.

\subsubsection{Bennett Acceptance Ratio}
Although TI is a good alternative, obtaining the free energy is very different from the EXP 
method. If the difference in total energy is preferred than the derivative, the 
Bennett-Acceptance-Ratio (BAR)~\cite{Bennett1976} can be used instead to get a much better 
estimate of the free energy. In essence, the method requires the calculation of the potential 
energy difference $\Delta U=U_{B}-U_{A}$ between two neighbouring lambda points like in 
Eq.~\eqref{eq:zwanzig} or between the forward and backward transformation at $\lambda_{i}$. 
The free energy difference is recovered by calculating the logarithm of the ratio of the 
partition function $Q$ between the two states, i.e. 
\begin{equation}
\Delta G = -\kT \,\text{ln}\frac{Q_{1}}{Q_{0}}.
\label{eq:barQ}
\end{equation}
The partition function is obtained by using a maximum likelihood estimator of the potential 
energy difference and takes the form of the Fermi-Dirac distribution
\begin{equation}
f(x) = \frac{1}{1+ \text{exp}(x)}.
\label{eq:fermi}
\end{equation}
Considering Eqs.~\eqref{eq:barQ} and \eqref{eq:fermi}, the free energy difference is estimated 
using the equation below
\begin{equation}
\Delta G = -\kT \,\text{ln} \left[ \frac{\langle f[-(\Delta U-C)/k_{B}T] \rangle_{1}}{\langle f[(\Delta U-C)/k_{B}T] \rangle_{0}} \right].
\label{eq:BAR}
\end{equation}
The constant $C$ and the free energy $\Delta G$ are two unknown variables; thus 
Eq~\eqref{eq:BAR} is solved iteratively. The constant $C$ is initially given a value of zero, 
and the free energy $\Delta G$ is calculated using the equation above, which takes in the data 
from the alchemical simulation. The calculated value of $\Delta G$ in step $i$ is used as the 
value for $C$ in the next step $i+1$, i.e.
\begin{equation}
    C_{i+1} = \Delta G_{i} .
\end{equation}
This process is repeated until the value of $\Delta G_{i}$ and $C_{i-1}$ is equal to each other 
(or numerically the same up to a given tolerance value like $1 \times 10^{-5}$). A BAR estimator 
called \verb+ParseFEP+~\cite{Liu2012} is implemented as a VMD plugin and can be used to process 
files generated by NAMD. BAR gives a more accurate estimate of $\Delta G$ than EXP since both 
forward and backward transformations at each $\lambda$ point are considered in the estimator. 
In comparison, EXP only considers the average of the endpoints (i.e. at $\lambda=1$) when 
combining the forward and backward data.

\subsubsection{Multistate Bennett Acceptance Ratio}
\begin{figure}[t!]
\centering
\includegraphics[width=15cm]{Figures/Method/MBAR-Potential.png}
\caption{Diagram illustrating the reduced potential $u(\bm{x})$ used in 
free-energy calculations with MBAR. $K$ is the number of $\lambda$ states 
and $N$ is the number of frames. The rows represent $\Delta U$ at each 
$\lambda$ states and the columns represent $\Delta U$ at $\lambda_{k}$ 
with the coordinates at $\lambda_{j}$. (Image is designed in Tikz)}
\label{method:MBAR}
\end{figure}
The BAR method, as described previously, can be generalised to include all states from 
$\lambda=0$ to 1 using the multistate Bennett-Acceptance-Ratio (MBAR)~\cite{Shirts2008}. MBAR 
reduces to BAR when only two states are considered and hence the given name. In this method, 
a {\it reduced potential} for thermodynamic state $i$ is defined depending on the ensemble of 
the system
\begin{equation}
u_{i}(\bm{x}) = [U_{i}(\bm{x}) + p_{i}V(\bm{x}) + \mu_{i}^T n(\bm{x})]/\kT,
\end{equation}
where the first term is the potential energy function and the second and third terms are 
energies due to pressure and chemical potentials, respectively. To generate the reduced potential 
$u(\bm{x})$ in FEP calculations, the results from different states are combined. At $\lambda_{j}$, 
the energy of the system is calculated in the same way as in the EXP method. However, the energy 
at different $\lambda$ states is also calculated using the same coordinates used to calculate 
the energy at $\lambda_{j}$. This procedure is continued for the next time step up to $N$ 
trajectory steps and repeated for the next state $\lambda_{j+1}$ up to $K$ states. 
\figref{method:MBAR} gives an illustration of the reduced potential in matrix form. Once 
$u(\bm{x})$ is defined, the free energy is recovered using the equation below\footnote{In the 
original derivation the free energy is unitless and written with the symbol $f$.}
\begin{equation}
\Delta G_{i} = -\kT \,\text{ln} \sum_{j=1}^{K}\sum_{n=1}^{N_{j}} \frac{\text{exp}[-u_{i}(\bm{x}_{jn})]}{\sum_{k=1}^{K} \text{exp}[\Delta G_{k}-u_{k}(\bm{x}_{jn})]},
\label{eq:MBAR}
\end{equation}
where $j$ and $k$ are indices over $\lambda$ states and $n$ is an index over frames/trajectory 
data. Similar to BAR, $\Delta G_{k}$ is unknown initially and is solved with an iterative method. 
The final free energy from $\lambda=0$ to 1 is calculated using Eq.~\eqref{eq:sumG}. A python 
implementation called \textit{pymbar} is available from 
\url{https://github.com/choderalab/pymbar}. Since the results from all states are combined, MBAR 
is regarded as a much better estimator than EXP or BAR and gives the smallest variance compared 
to all other FEP methods~\cite{Shirts2008}. However, this can be computationally expensive if many 
states are considered, and the data is large. In this case, it is better to use the uncorrelated 
data in Eq.~\eqref{eq:MBAR} instead and is determined with the \verb+timeseries+ module as part 
of the \textit{pymbar} package~\cite{Chodera2016}.

\subsection{Potential of Mean Force}
\label{sec:PMF}
\begin{figure}[b!]
\centering
\includegraphics[width=1.0\textwidth]{Figures/Method/Umbrella-Sampling.png}
\caption{A molecule is moved from point A to B by applying an external harmonic 
potential. The path is discretised in separate simulations, and the PMF can be 
constructed by solving the WHAM equations. (Image is designed in Tikz)}
\label{method:US}
\end{figure}
When performing path-dependent free energy calculations, we are interested in describing the 
potential energy surface of some process. On this end, path-dependent methods require the 
determination of the potential of mean force (PMF). In simple terms, this is the work done 
$W$ for moving a particle from point A to B along some reaction coordinate\footnote{In some 
literature text this is referred to as collective variables.},
\begin{equation}
PMF(\xi) = W(\xi) = \int_{A}^{B} \langle F(\xi) \rangle_{\xi} \, d\xi,
\end{equation}
where $\xi$ is the reaction coordinate and $\langle F(\xi) \rangle_{\xi}$ is the average or 
mean force at point $\xi$. Using protein-ligand as an example, the ligand can escape the binding 
site if we simulate to experimental time scales. The PMF can then be obtained by 
constructing a histogram distribution $\rho(\xi)$ along the reaction coordinate
\begin{equation}
PMF(\xi) = -\kT \, \text{ln}[\rho(\xi)] + C,
\end{equation}
where $C$ is a constant value\footnote{$C$ is an offset as a result of integration}. However, 
experimental time scales are still not reachable with the current hardware. Therefore, to 
construct the PMF, we need to force the ligand out of the binding site by applying a ``bias" 
that is corrected {\it a posteriori}. The most commonly used method for biasing a molecule over 
a reaction coordinate is the umbrella sampling (US) method~\cite{Torrie1977}. In essence, the 
path from \textbf{A} to \textbf{B} is discretised into multiple windows (stratification) by 
applying an external potential. The external potential is usually a harmonic potential as it 
is easier to work with due to its mathematical properties. The external harmonic potential 
results in a Gaussian distribution of the ligand's centre of mass (COM) on the reaction 
coordinate. The PMF profile can be recovered from the biased distributions using the weighted 
histogram analysis method (WHAM)\footnote{WHAM is just one method to recover the PMF}~\cite{Kumar1992} 
(see \figref{method:US}). The biased distributions are combined using what is referred to as 
the WHAM equations
\begin{subequations}
\begin{align}
\rho^{(unbiased)}(\xi) &= \frac{\sum\limits_{i=1}^{N_{w}} n_{i}\rho_{i}^{(biased)}(\xi)}
{\sum\limits_{j=1}^{N_{w}}n_{j}e^{-(V_{j}(\xi)-\Delta G_{j})/\kT}}, \\ 
e^{-\Delta G_{i}/\kT} &= \int e^{-V_{i}(\xi)/\kT}\,\rho^{(unbiased)}(\xi)\, d\xi.
\end{align}
\end{subequations}
Here $N_{w}$ is the number of windows, $n$ is the number of samples at each window, $V(\xi)$ is 
the harmonic biasing potential, and $\rho^{(biased)}$ and $\rho^{(unbiased)}$ are the biased and 
unbiased probability distributions respectively. Since $\Delta G$ is the unknown quantity present 
in both equations, the WHAM equations are solved iteratively until convergence is reached. A C$++$ 
implementation of the WHAM equations is distributed by the Grossfield Lab~\cite{Grossfield2013}. 
To ensure convergence of the PMF, the distributions of the reaction coordinate(s) between 
neighbouring windows must have sufficient overlap between them. In a study of ChTx toxin-binding 
to KcsA ion channel 5\% overlap between neighbouring windows was found to be sufficient~\cite{Chen2011}.
However, different systems might require different criteria for convergence. There are several 
other methods available that can give an estimate of the PMF like the adaptive-biasing force 
(ABF)~\cite{Darve2008} and metadynamics~\cite{Laio2008}. These methods are based on non-equilibrium 
statistical mechanics, while US-WHAM is based on equilibrium simulations. All of these methods 
essentially changes the underlying potential energy to speed up a slow process that 
otherwise cannot be observed by brute-force MD simulations alone.
%=======================================================================================%